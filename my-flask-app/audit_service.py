# my-flask-app/audit_service.py
import os
import asyncio
import aiofiles
from datetime import datetime
import json
import aiohttp
import re
from pathlib import Path
from search_project_data import ProjectDocumentSearcher
from gemini import analyze_with_gemini, clear_analysis_cache
from config import PROJECT_LIST_CSV, NETWORK_BASE_PATH, DISCORD_WEBHOOK_URL, STATIC_DATA_PATH, get_full_path, AUDIT_FILTERS, AUDIT_FILTERS_depart
from config_assets import DOCUMENT_TYPES, DEPARTMENT_MAPPING, DEPARTMENT_NAMES
import traceback
import time
import logging
import pandas as pd
import orjson
from functools import lru_cache
from diskcache import Cache
import tempfile
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
import psutil
from gemini import DocumentAnalyzer

# JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
RESULTS_DIR = os.path.join(os.path.dirname(STATIC_DATA_PATH), 'results')
os.makedirs(RESULTS_DIR, exist_ok=True)

logger = logging.getLogger(__name__)

class AuditService:
    def __init__(self):
        self.searcher = ProjectDocumentSearcher(verbose=False)
        cache_dir = os.path.join(tempfile.gettempdir(), 'audit_cache')
        self._cache = Cache(directory=cache_dir, size_limit=1024*1024*1024)  # 1GB ìºì‹œ
        self._session = None
        self._last_request_time = 0  # AI ìš”ì²­ ì‹œê°„ ì¶”ì ìš©
        cpu_count = multiprocessing.cpu_count()
        self.executor = ThreadPoolExecutor(max_workers=min(32, max(4, cpu_count * 2)))
        self.document_analyzer = DocumentAnalyzer()
        
    async def _get_session(self):
        """aiohttp ì„¸ì…˜ ê´€ë¦¬"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()
        return self._session

    async def save_audit_result(self, result, department_code):
        try:
            logger.debug("\n[DEBUG] === Saving Audit Result to JSON ===")
            project_id = result['project_id']
            
            # ë¶€ì„œ ì½”ë“œë¡œ íŒŒì¼ëª… ìƒì„±
            filename = f"audit_{department_code}_{project_id}.json"
            filepath = Path(RESULTS_DIR) / filename
            
            logger.debug(f"[DEBUG] Saving to: {filepath}")
            if not os.path.exists(RESULTS_DIR):
                os.makedirs(RESULTS_DIR, exist_ok=True)
            elif filepath.parent != Path(RESULTS_DIR) and not os.path.exists(filepath.parent):
                os.makedirs(filepath.parent, exist_ok=True)
            
            json_data = orjson.dumps(result, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS).decode('utf-8')
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(json_data)
            
            logger.debug(f"[DEBUG] JSON file saved successfully")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"[DEBUG] Error saving JSON file: {str(e)}")
            return None

    async def send_progress_message(self, ctx, message):
        """ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì „ì†¡"""
        session = await self._get_session()
        try:
            if not DISCORD_WEBHOOK_URL:
                logger.warning("[DEBUG] No Discord webhook URL configured, using console output")
                print(message)
                return
            
            # Webhookìœ¼ë¡œ ìš°ì„  ì „ì†¡, íƒ€ì„ì•„ì›ƒ ì„¤ì •
            try:
                async with session.post(DISCORD_WEBHOOK_URL, json={'content': message}, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 204:
                        logger.debug("[DEBUG] Message sent via webhook successfully")
                    else:
                        logger.warning(f"[DEBUG] Webhook response status: {response.status}, Response: {await response.text()}")
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.error(f"[DEBUG] Webhook error: {str(e)}, falling back to console output")
                print(message)
            
            # ctxê°€ ìœ íš¨í•œ ê²½ìš° ì¶”ê°€ ì „ì†¡
            if ctx and hasattr(ctx, 'send'):
                await ctx.send(message)
                logger.debug("[DEBUG] Message sent via ctx.send")
            else:
                logger.debug("[DEBUG] Using webhook or console for progress message")
                
        except Exception as e:
            logger.error(f"[DEBUG] Progress message error: {str(e)}, using console output")
            print(message)

    @lru_cache(maxsize=1)
    def _load_project_df(self):
        return pd.read_csv(PROJECT_LIST_CSV, dtype={'department_code': str, 'project_id': str})

    async def get_project_info_by_dept(self, project_id, department_code=None):
        try:
            cache_key = f"project_info:{project_id}:{department_code}"
            cached_result = self._cache.get(cache_key)
            if cached_result:
                logger.debug(f"Cache hit for project info: {project_id}")
                return cached_result

            numeric_project_id = re.sub(r'[^0-9]', '', str(project_id))
            df = pd.read_csv(PROJECT_LIST_CSV, dtype={'department_code': str, 'project_id': str})
            
            if department_code:
                department_code = str(department_code).zfill(5)
            
            # ê¸°ë³¸ ê²€ìƒ‰
            if department_code:
                project = df[(df['project_id'] == numeric_project_id) & 
                           (df['department_code'].str.zfill(5) == department_code)]
            else:
                project = df[df['project_id'] == numeric_project_id]
            
            if len(project) == 0:
                # ëŒ€ì²´ ê²€ìƒ‰ (ë¶€ì„œ ì½”ë“œ ì—†ì´)
                alt_project = df[df['project_id'] == numeric_project_id]
                if len(alt_project) > 0:
                    logger.warning(f"Department code {department_code} not found for project {numeric_project_id}, using first match")
                    project = alt_project.iloc[0:1]
                else:
                    logger.error(f"Project ID {numeric_project_id} not found in project list")
                    return None
            
            row = project.iloc[0]
            dept_code = str(row['department_code']).zfill(5)
            dept_name = str(row['department_name'])
            logger.debug(f"[DEBUG] Found project - ID: {numeric_project_id}, Dept: {dept_code}, Name: {dept_name}")
            
            # ë„¤íŠ¸ì›Œí¬ ë“œë¼ì´ë¸Œ ìƒíƒœ í™•ì¸ ë° ì¬ì‹œë„
            retry_count = 3
            for attempt in range(retry_count):
                if os.path.exists(NETWORK_BASE_PATH):
                    break
                logger.warning(f"Network drive {NETWORK_BASE_PATH} not accessible, attempt {attempt + 1}/{retry_count}")
                await asyncio.sleep(1)
            
            if not os.path.exists(NETWORK_BASE_PATH):
                raise FileNotFoundError(f"Network drive {NETWORK_BASE_PATH} remains inaccessible after {retry_count} attempts")
            
            original_folder = str(row['original_folder'])
            full_path = get_full_path(original_folder, check_exists=False)
            logger.debug(f"[DEBUG] Original path: {original_folder}")
            logger.debug(f"[DEBUG] Full path: {full_path}")
            
            result = {
                'project_id': str(row['project_id']),
                'department_code': dept_code,
                'department_name': dept_name,
                'project_name': str(row['project_name']),
                'original_folder': full_path
            }
            
            # ê²°ê³¼ ìºì‹± (1ì‹œê°„)
            self._cache.set(cache_key, result, expire=3600)
            return result
            
        except Exception as e:
            logger.error(f"Error in get_project_info_by_dept for {project_id}: {str(e)}")
            logger.debug(f"Debug info - Project ID: {project_id}, Department: {department_code}, CSV: {PROJECT_LIST_CSV}")
            return None

    async def audit_project(self, project_id, department_code=None, use_ai=False, ctx=None):
        start_time = time.time()
        session = None
        try:
            session = await self._get_session()
            numeric_project_id = re.sub(r'[^0-9]', '', str(project_id))
            logger.info(f"\n=== í”„ë¡œì íŠ¸ {project_id} (ID: {numeric_project_id}) ê°ì‚¬ ì‹œì‘ ===")
            
            # í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ (ìºì‹œ í™œìš©)
            project_info = await self.get_project_info_by_dept(project_id, department_code)
            if not project_info:
                raise ValueError(f'Project ID {project_id} not found for department {department_code}')
            
            # ì§„í–‰ ìƒí™© ë©”ì‹œì§€
            progress_msg = f"ğŸ” í”„ë¡œì íŠ¸ ê°ì‚¬ ì§„í–‰ ì¤‘: {project_info['project_name']} ({project_info['department_name']})"
            await self.send_progress_message(ctx, progress_msg)
            
            # ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰
            search_start = time.time()
            search_result = await self.searcher.search_all_documents(
                project_info['project_id'],
                project_info['department_code']
            )
            search_time = time.time() - search_start
            
            # ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬
            if not search_result or 'documents' not in search_result:
                logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_id}, Result: {search_result}")
                raise ValueError(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_id}")
            
            documents = search_result['documents']
            performance = search_result.get('performance', {'search_time': search_time, 'document_counts': {}})
            
            # ë¬¸ì„œ ìœ í˜• ë° íŒŒì¼ ìˆ˜ ìš”ì•½
            found_count = sum(1 for docs in documents.values() if docs['exists'])
            total_files = sum(len(docs['details']) for docs in documents.values() if docs['exists'])
            
            logger.info(f"\n=== ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({search_time:.2f}ì´ˆ) ===")
            logger.info(f"- ë°œê²¬ëœ ë¬¸ì„œ ìœ í˜•: {found_count}/{len(DOCUMENT_TYPES)}ê°œ")
            logger.info(f"- ì´ ë°œê²¬ íŒŒì¼ ìˆ˜: {total_files}ê°œ")
            
            # ê° ë¬¸ì„œ ìœ í˜•ë³„ ìƒì„¸ ë¡œê·¸
            for doc_type in documents:
                if documents[doc_type]['exists']:
                    logger.info(f"{doc_type}: {len(documents[doc_type]['details'])}ê°œ ë°œê²¬ ({performance.get('document_counts', {}).get(doc_type, 0):.2f}ì´ˆ)")
            
            # AI ë¶„ì„ (ì˜µì…˜)
            ai_analysis = None
            ai_time = 0
            if use_ai:
                logger.info("\n=== AI ë¶„ì„ ì‹œì‘ ===")
                ai_start = time.time()
                try:
                    ai_input = {
                        'project_id': project_id,  # project_id ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                        'project_info': project_info,
                        'documents': documents
                    }
                    ai_analysis = await analyze_with_gemini(ai_input, session)
                    ai_time = time.time() - ai_start
                    logger.info(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ===")
                except Exception as ai_err:
                    logger.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {str(ai_err)}")
                    ai_analysis = f"AI analysis failed: {str(ai_err)}"
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'project_id': project_info['project_id'],
                'project_name': project_info['project_name'],
                'department': f"{project_info['department_code']}_{project_info['department_name']}",
                'documents': documents,
                'original_folder': project_info['original_folder'],  # ê²½ë¡œ ì¶”ê°€
                'ai_analysis': ai_analysis if use_ai else None,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'performance': {
                    'total_time': time.time() - start_time,
                    'search_time': search_time,
                    'ai_time': ai_time,
                    'save_time': 0,
                    'metrics': {
                        'network_io_time': search_time,
                        'cache_hits': getattr(self.searcher, 'cache_hits', 0),
                        'cache_misses': getattr(self.searcher, 'cache_misses', 0),
                        'file_count': total_files,
                        'memory_usage': psutil.Process().memory_info().rss
                    }
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            save_start = time.time()
            json_path = await self.save_audit_result(result, project_info['department_code'])
            if json_path:
                result['json_file'] = json_path
                result['performance']['save_time'] = time.time() - save_start
                logger.info(f"\n=== ê²°ê³¼ ì €ì¥ ì¤‘ ===\nê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
            
            # Discordë¡œ ê²°ê³¼ ì „ì†¡
            await self.send_to_discord(result, ctx)
            
            logger.info(f"\n=== ê°ì‚¬ ì™„ë£Œ ({result['performance']['total_time']:.2f}ì´ˆ) ===")
            logger.info("ì„±ëŠ¥ ìš”ì•½:")
            logger.info(f"- ì´ ì†Œìš” ì‹œê°„: {result['performance']['total_time']:.2f}ì´ˆ")
            logger.info(f"- ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„: {search_time:.2f}ì´ˆ")
            logger.info(f"- AI ë¶„ì„ ì‹œê°„: {ai_time:.2f}ì´ˆ")
            logger.info(f"- JSON ì €ì¥ ì‹œê°„: {result['performance']['save_time']:.2f}ì´ˆ")
            logger.info(f"- ë°œê²¬ëœ ë¬¸ì„œ ìœ í˜•: {found_count}/{len(DOCUMENT_TYPES)}ê°œ")
            logger.info(f"- ì´ ë°œê²¬ íŒŒì¼ ìˆ˜: {total_files}ê°œ")
            logger.info(f"- Cache stats: Hits={result['performance']['metrics']['cache_hits']}, Misses={result['performance']['metrics']['cache_misses']}")

            return result
            
        except Exception as e:
            error_msg = f"í”„ë¡œì íŠ¸ {project_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            error_result = {
                'error': str(e),
                'project_id': project_id,
                'department_code': department_code,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'performance': {
                    'total_time': time.time() - start_time
                }
            }
            await self.send_to_discord(error_result, ctx)
            return error_result
        finally:
            if session and not session.closed:
                await session.close()

    async def audit_multiple_projects(self, project_ids, department_codes, use_ai=False, ctx=None):
        """ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ë°°ì¹˜ ê°ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in project_ids]
        tasks = []
        
        for pid, dept in zip(numeric_project_ids, department_codes):
            task = asyncio.create_task(self.audit_project(pid, dept, use_ai, ctx))
            tasks.append(task)
        
        # ê²°ê³¼ ìˆ˜ì§‘ ë° ìºì‹œ í†µê³„
        results = []
        cache_stats = {'hits': 0, 'misses': 0}
        
        for task in asyncio.as_completed(tasks):
            result = await task
            results.append(result)
            if 'performance' in result and 'metrics' in result['performance']:
                cache_stats['hits'] += result['performance']['metrics'].get('cache_hits', 0)
                cache_stats['misses'] += result['performance']['metrics'].get('cache_misses', 0)
        
        logger.info(f"Cache statistics - Hits: {cache_stats['hits']}, Misses: {cache_stats['misses']}")
        return results

    async def send_to_discord(self, data, ctx=None):
        """ë””ìŠ¤ì½”ë“œë¡œ ê²°ê³¼ ì „ì†¡"""
        try:
            logger.debug("\n[DEBUG] === Sending to Discord ===")
            
            if 'error' in data:
                project_id = data.get('project_id', 'Unknown')
                logger.debug(f"[DEBUG] Sending error message for project {project_id}")
                message = (
                    f"âŒ **Audit Error**\n"
                    f"Project ID: {project_id}\n"
                    f"Error: {data['error']}\n"
                    f"Time: {data.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}"
                )
            else:
                message = (
                    f"ğŸ“‹ **Project Audit Result**\n"
                    f"ID: {data['project_id']}\n"
                    f"Department: {data['department']}\n"
                    f"Name: {data['project_name']}\n"
                    f"Path: {data['original_folder']}\n\n"  # original_folder ì¶”ê°€
                    f"ğŸ“‘ Documents:\n"
                )

                found_docs = []
                missing_docs = []
                for doc_type, info in data['documents'].items():
                    doc_name = DOCUMENT_TYPES[doc_type]['name']
                    if info['exists']:
                        found_docs.append(f"{doc_name} ({len(info['details'])}ê°œ)")
                    else:
                        missing_docs.append(doc_name)

                if found_docs:
                    message += "âœ… Found:\n- " + "\n- ".join(found_docs) + "\n\n"
                if missing_docs:
                    message += "âŒ Missing:\n- " + "\n- ".join(missing_docs) + "\n\n"

                if 'ai_analysis' in data:
                    message += f"ğŸ¤– AI Analysis:\n{data['ai_analysis']}\n\n"

                if 'json_file' in data:
                    message += f"\nğŸ’¾ Results saved to: {data['json_file']}"

                message += f"\nâ° {data['timestamp']}"

            session = await self._get_session()
            try:
                # Webhookìœ¼ë¡œ ìš°ì„  ì „ì†¡, íƒ€ì„ì•„ì›ƒ ì„¤ì •
                if DISCORD_WEBHOOK_URL:
                    try:
                        async with session.post(DISCORD_WEBHOOK_URL, json={'content': message}, timeout=aiohttp.ClientTimeout(total=10)) as response:
                            if response.status == 204:
                                logger.debug("[DEBUG] Message sent via webhook successfully")
                            else:
                                logger.warning(f"[DEBUG] Webhook response status: {response.status}, Response: {await response.text()}")
                    except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                        logger.error(f"[DEBUG] Webhook error: {str(e)}, falling back to console output")
                        print(message)
                
                # ctxê°€ ìœ íš¨í•œ ê²½ìš° ì¶”ê°€ ì „ì†¡
                if ctx and hasattr(ctx, 'send'):
                    await ctx.send(message)
                    logger.debug("[DEBUG] Message sent via ctx.send")
                else:
                    logger.debug("[DEBUG] Using webhook or console for result message")
                    
            except Exception as e:
                logger.error(f"[DEBUG] Error sending Discord message: {str(e)}, using console output")
                print(message)
            finally:
                if session:
                    await session.close()
                    
        except Exception as e:
            logger.error(f"[DEBUG] Error in send_to_discord: {str(e)}, using console output")
            print(f"âŒ Error sending message: {str(e)}")

    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self.searcher.clear_cache()
        self.document_analyzer.clear_cache()
        logger.info("All caches cleared")

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._session and not self._session.closed:
            await self._session.close()
        self.executor.shutdown(wait=True)
        self._cache.close()
        await self.document_analyzer.close()
        logger.info("AuditService resources cleaned up")

    async def generate_summary_report(self, results):
        """ì „ì²´ ê°ì‚¬ ê²°ê³¼ì— ëŒ€í•œ ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        try:
            logger.info("\n[DEBUG] === Generating Summary Report ===")
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            summary_filename = f"summary_report_{timestamp}.json"
            summary_path = os.path.join(RESULTS_DIR, summary_filename)
            
            summary = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_projects': len(results),
                'successful_audits': sum(1 for r in results if 'error' not in r),
                'failed_audits': sum(1 for r in results if 'error' in r),
                'document_statistics': {
                    doc_type: {
                        'found': 0,
                        'missing': 0
                    } for doc_type in DOCUMENT_TYPES.keys()
                },
                'department_statistics': {},
                'detailed_results': results
            }
            
            for result in results:
                if 'error' not in result:
                    dept = result['department']
                    if dept not in summary['department_statistics']:
                        summary['department_statistics'][dept] = {
                            'total': 0,
                            'documents_found': 0,
                            'documents_missing': 0
                        }
                    
                    dept_stats = summary['department_statistics'][dept]
                    dept_stats['total'] += 1
                    
                    for doc_type, info in result['documents'].items():
                        if info['exists']:
                            summary['document_statistics'][doc_type]['found'] += 1
                            dept_stats['documents_found'] += 1
                        else:
                            summary['document_statistics'][doc_type]['missing'] += 1
                            dept_stats['documents_missing'] += 1
            
            if not os.path.exists(RESULTS_DIR):
                os.makedirs(RESULTS_DIR, exist_ok=True)
            json_data = orjson.dumps(summary, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS).decode('utf-8')
            async with aiofiles.open(summary_path, 'w', encoding='utf-8') as f:
                await f.write(json_data)
            
            logger.info(f"[DEBUG] Summary report saved to: {summary_path}")
            return summary_path, summary
            
        except Exception as e:
            logger.error(f"[ERROR] Failed to generate summary report: {str(e)}")
            return None, None

    async def process_audit_targets(self, filters=None, use_ai=False, ctx=None):
        """ê°ì‚¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°°ì¹˜ë¡œ ê°ì‚¬ ìˆ˜í–‰"""
        from audit_target_generator import select_audit_targets  # ë™ì  ì„í¬íŠ¸

        try:
            # ê°ì‚¬ ëŒ€ìƒ ìƒì„±
            audit_targets_df, project_ids, department_codes = select_audit_targets(filters)
            
            # project_idsì—ì„œ ì•ŒíŒŒë²³ ì œê±°
            numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in project_ids]
            
            # ë°°ì¹˜ ê°ì‚¬ ì‹¤í–‰
            results = await self.audit_multiple_projects(numeric_project_ids, department_codes, use_ai, ctx)
            
            # ê°ì‚¬ ê²°ê³¼ audit_targets_dfì— ì¶”ê°€
            audit_targets_df['AuditResult'] = [
                result.get('ai_analysis', 'No result') if 'error' not in result else f"Error: {result['error']}" 
                for result in results
            ]
            
            # ê²°ê³¼ ì €ì¥ (audit_results.csv)
            output_csv = os.path.join(STATIC_DATA_PATH, 'audit_results.csv')
            audit_targets_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
            
            logger.info(f"ê°ì‚¬ ê²°ê³¼ê°€ {output_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ ìˆ˜: {len(audit_targets_df)}")
            
            # Discordë¡œ ìš”ì•½ ë©”ì‹œì§€ ì „ì†¡
            summary_message = (
                f"ğŸ“Š **Audit Summary**\n"
                f"Total Projects: {len(audit_targets_df)}\n"
                f"Successful: {sum(1 for r in results if 'error' not in r)}\n"
                f"Failed: {sum(1 for r in results if 'error' in r)}"
            )
            await self.send_progress_message(ctx, summary_message)
            
            return audit_targets_df, results
            
        except Exception as e:
            logger.error(f"ê°ì‚¬ ëŒ€ìƒ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            await self.send_progress_message(ctx, f"âŒ Audit processing error: {str(e)}")
            return None, None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰")
    parser.add_argument('--project-id', type=str, nargs='+', required=False, help="ê²€ìƒ‰í•  í”„ë¡œì íŠ¸ ID (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)")
    parser.add_argument('--department-code', type=str, nargs='+', default=None, help="ë¶€ì„œ ì½”ë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥, ì˜ˆ: 01010, 06010)")
    parser.add_argument('--use-ai', action='store_true', help="AI ë¶„ì„ ì‚¬ìš©")
    parser.add_argument('--year', type=int, nargs='+', default=AUDIT_FILTERS['year'], help="í•„í„°ë§í•  ì¤€ê³µ ì—°ë„ (ê¸°ë³¸ê°’: 2024)")
    parser.add_argument('--status', type=str, nargs='+', default=['ì§„í–‰', 'ì¤‘ì§€'], help="í•„í„°ë§í•  ì§„í–‰ ìƒíƒœ (ê¸°ë³¸ê°’: ì§„í–‰, ì¤‘ì§€)")
    parser.add_argument('--is-main-contractor', type=str, nargs='+', default=AUDIT_FILTERS['is_main_contractor'], help="í•„í„°ë§í•  ì£¼ê´€ì‚¬ ì—¬ë¶€ (ê¸°ë³¸ê°’: ì£¼ê´€ì‚¬, ë¹„ì£¼ê´€ì‚¬)")
    parser.add_argument('--department', type=str, nargs='+', default=AUDIT_FILTERS['department'] or AUDIT_FILTERS_depart, help="í•„í„°ë§í•  ë¶€ì„œ ì½”ë“œ (ê¸°ë³¸ê°’: 01010, 06010)")
    args = parser.parse_args()
    
    if args.use_ai:
        logging.getLogger().setLevel(logging.INFO)
    
    logger.info("=== í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ===")
    service = AuditService()
    
    if args.project_id:
        # project_idsì—ì„œ ì•ŒíŒŒë²³ ì œê±°
        numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in args.project_id]
        department_codes = args.department_code if args.department_code else [None] * len(numeric_project_ids)
        if len(department_codes) != len(numeric_project_ids):
            raise ValueError("ë¶€ì„œ ì½”ë“œì™€ í”„ë¡œì íŠ¸ IDì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ë‹¨ì¼ ë˜ëŠ” ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ê°ì‚¬
        if len(numeric_project_ids) == 1:
            asyncio.run(service.audit_project(args.project_id[0], department_codes[0] if department_codes[0] else None, args.use_ai))
        else:
            asyncio.run(service.audit_multiple_projects(numeric_project_ids, department_codes, args.use_ai))
    else:
        # í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ê°ì‚¬ ëŒ€ìƒ ìƒì„± ë° ë°°ì¹˜ ê°ì‚¬
        filters = {
            'year': args.year,
            'status': args.status,
            'is-main-contractor': args.is_main_contractor,
            'department': args.department
        }
        asyncio.run(service.process_audit_targets(filters, args.use_ai))
    
    logger.info("\n=== ê²€ìƒ‰ ì™„ë£Œ ===")
    asyncio.run(service.close())
    
# python audit_service.py
# python audit_service.py --project-id 20180076 --department-code 01010 --use-ai
# python audit_service.py --project-id 20240178 --department-code 06010 --use-ai