# -*- coding: utf-8 -*-

# my_flask_app/gemini.py
import google.generativeai as genai
import os
import asyncio
import hashlib
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict
from functools import lru_cache
from config import GOOGLE_API_KEY, DISCORD_WEBHOOK_URL, DOCUMENT_TYPES, AUDIT_FILTERS
import requests
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Gemini ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# ìºì‹œ ë° rate limit ì„¤ì •
_analysis_cache = {}
_rate_limit = asyncio.Semaphore(5)  # Gemini API ì œí•œì— ë§ì¶° ì¡°ì • (ì´ˆë‹¹ 5ê±´ ê°€ì •)
_last_request_time = {}
MIN_REQUEST_INTERVAL = 0.2  # ì´ˆë‹¹ 5ê°œ ìš”ì²­ì— ë§ì¶˜ ê°„ê²©
MAX_RETRIES = 3  # ì¬ì‹œë„ íšŸìˆ˜

class DocumentAnalyzer:
    def __init__(self):
        self._session = None
        self._cache = {}
        self._last_request_time = {}  # ì¸ìŠ¤í„´ìŠ¤ ì†ì„±ìœ¼ë¡œ ë³€ê²½
        
    async def get_session(self):
        """ë¹„ë™ê¸° HTTP ì„¸ì…˜ ê´€ë¦¬"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()
        return self._session

    def calculate_risk_score(self, missing_docs: tuple, status: str, contractor: str) -> int:
        """ë¬¸ì„œ ëˆ„ë½, ìƒíƒœ, ì£¼ê´€ì‚¬/ë¹„ì£¼ê´€ì‚¬ì— ë”°ë¥¸ ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° (ì˜¤í”„ë¼ì¸)"""
        base_score = 100
        risk_weights = {
            'contract': 30,    # ê³„ì•½ì„œ
            'specification': 25,  # ê³¼ì—…ì§€ì‹œì„œ
            'budget': 20,      # ì‹¤í–‰ì˜ˆì‚°
            'completion': 15,   # ì¤€ê³µê³„
            'certificate': 10,  # ì‹¤ì ì¦ëª…
            'evaluation': 5,    # ìš©ì—­ìˆ˜í–‰í‰ê°€
            'initiation': 5,    # ì°©ìˆ˜ê³„
            'agreement': 5,     # ê³µë™ë„ê¸‰í˜‘ì •
            'deliverable1': 5,  # ì„±ê³¼í’ˆ(ë³´ê³ ì„œ)
            'deliverable2': 5   # ì„±ê³¼í’ˆ(ë„ë©´)
        }
        
        # ì£¼ê´€ì‚¬/ë¹„ì£¼ê´€ì‚¬ ê¸°ì¤€
        required_docs = []
        if contractor == 'ì£¼ê´€ì‚¬':
            required_docs = list(risk_weights.keys())  # ëª¨ë“  ë¬¸ì„œ í•„ìš”
        elif contractor == 'ë¹„ì£¼ê´€ì‚¬':
            required_docs = ['contract', 'agreement', 'deliverable1', 'deliverable2']  # ìµœì†Œ í•„ìš” ë¬¸ì„œ

        # ì§„í–‰/ì¤€ê³µ ê¸°ì¤€
        if status == 'ì¤€ê³µ':
            required_docs = list(risk_weights.keys())  # ëª¨ë“  ë¬¸ì„œ í•„ìš”
        elif status == 'ì§„í–‰':
            required_docs = ['contract', 'specification', 'initiation', 'agreement', 'budget']  # ìš°ì„  í•„ìš” ë¬¸ì„œ

        # ëˆ„ë½ëœ ë¬¸ì„œì— ë”°ë¥¸ ìœ„í—˜ë„ ê³„ì‚° (ì‹¤ì œ ì¡´ì¬ í™•ì¸)
        for doc in missing_docs:
            if doc in risk_weights and doc in required_docs:
                base_score -= risk_weights[doc]
        
        return max(0, min(100, base_score))  # 0~100 ë²”ìœ„ë¡œ ì œí•œ

    def _generate_cache_key(self, project_id: str, documents: dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (ë¬¸ì„œ ìƒíƒœ í•´ì‹œ í¬í•¨, ë‚ ì§œ ì œê±°)"""
        docs_hash = hashlib.md5(str(documents).encode()).hexdigest()
        return f"{project_id}_{docs_hash}"

    async def _wait_for_rate_limit(self):
        """Rate limit ì¤€ìˆ˜ ë° ì¬ì‹œë„ ì²˜ë¦¬"""
        for attempt in range(MAX_RETRIES):
            async with _rate_limit:
                now = datetime.now()
                if self._last_request_time.get(os.getpid()):
                    elapsed = (now - self._last_request_time[os.getpid()]).total_seconds()
                    if elapsed < MIN_REQUEST_INTERVAL:
                        await asyncio.sleep(MIN_REQUEST_INTERVAL - elapsed)
                self._last_request_time[os.getpid()] = now
                return  # ì„±ê³µ ì‹œ ë°˜í™˜
            await asyncio.sleep(0.5 * (attempt + 1))  # ì¬ì‹œë„ ì „ ì§€ì—°
        raise Exception("Rate limit ì´ˆê³¼ ë˜ëŠ” ìš”ì²­ ì‹¤íŒ¨")

    async def _call_gemini_with_retry(self, prompt: str, max_retries=MAX_RETRIES) -> str:
        """Gemini API í˜¸ì¶œ ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(max_retries):
            try:
                await self._wait_for_rate_limit()
                response = await asyncio.to_thread(model.generate_content, prompt)
                return response.text
            except Exception as e:
                logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(1 * (attempt + 1))  # ì¬ì‹œë„ ì „ ì§€ì—°
                else:
                    raise

    async def analyze_batch(self, projects: List[Dict]) -> List[Dict]:
        """í”„ë¡œì íŠ¸ ë°°ì¹˜ ë¶„ì„"""
        tasks = [self.analyze_with_gemini(project) for project in projects]
        return await asyncio.gather(*tasks)

    async def analyze_with_gemini(self, project_data: dict, session: aiohttp.ClientSession = None) -> str:
        """í”„ë¡œì íŠ¸ ë¬¸ì„œ ë¶„ì„ì„ ìˆ˜í–‰"""
        try:
            # ì„¸ì…˜ ê´€ë¦¬
            if session is None:
                session = await self.get_session()
                should_close = True
            else:
                should_close = False

            # project_id, department, status, contractor, project_name, documents, csv_data ì¶”ì¶œ
            project_id = project_data.get('project_id', 'Unknown')
            department = project_data.get('department', 'Unknown')
            status = project_data.get('status', 'ì§„í–‰')
            contractor = project_data.get('contractor', 'ì£¼ê´€ì‚¬')
            project_name = project_data.get('project_name', 'Unknown')
            documents = project_data.get('documents', {})
            csv_data = project_data.get('csv_data', {})

            # ë””ë²„ê¹…: Gemini AIì— ì „ë‹¬ë˜ëŠ” ë°ì´í„° ì¶œë ¥
            logger.debug(f"Gemini AIì— ì „ë‹¬ë˜ëŠ” project_data: {project_data}")
            logger.debug(f"Gemini AIì— ì „ë‹¬ë˜ëŠ” documents: {documents}")
            logger.debug(f"Gemini AIì— ì „ë‹¬ë˜ëŠ” CSV ë°ì´í„°: {csv_data}")

            # ë¬¸ì„œ ìƒíƒœ ë¶„ì„ (documentsê°€ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬)
            if not isinstance(documents, dict):
                logger.error(f"Invalid documents format: {documents}")
                documents = {}  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”

            existing_docs = []
            missing_docs = []
            processed_documents = {}

            # ëª¨ë“  DOCUMENT_TYPESë¥¼ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
            for doc_type in DOCUMENT_TYPES.keys():
                doc_data = documents.get(doc_type, [])
                if isinstance(doc_data, list):  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ëœ ê²½ìš°
                    processed_documents[doc_type] = {
                        'exists': len(doc_data) > 0,
                        'details': [{'name': path} for path in doc_data if isinstance(path, str)]
                    }
                elif isinstance(doc_data, dict):  # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ëœ ê²½ìš°
                    processed_documents[doc_type] = {
                        'exists': doc_data.get('exists', False),
                        'details': [{'name': path} for path in doc_data.get('details', []) if isinstance(path, (str, dict))]
                    }
                else:
                    logger.warning(f"Unknown documents format for {doc_type}: {doc_data}")
                    processed_documents[doc_type] = {
                        'exists': False,
                        'details': []
                    }

                doc_name = DOCUMENT_TYPES.get(doc_type, {}).get('name', doc_type)
                if processed_documents[doc_type]['exists']:
                    files_count = len(processed_documents[doc_type]['details'])
                    existing_docs.append(f"{doc_name} ({files_count}ê°œ)")
                else:
                    missing_docs.append(f"{doc_name} (0ê°œ)")  # ë°œê²¬ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” 0ê°œë¡œ í‘œì‹œ

            # ë””ë²„ê¹…: ì²˜ë¦¬ëœ documents ì¶œë ¥
            logger.debug(f"Processed documents for project {project_id}: {processed_documents}")

            # ì˜¤í”„ë¼ì¸ ìœ„í—˜ë„ ê³„ì‚° (ìƒíƒœì™€ ì£¼ê´€ì‚¬/ë¹„ì£¼ê´€ì‚¬ ë°˜ì˜)
            risk_score = self.calculate_risk_score(tuple(doc_type for doc_type, info in processed_documents.items() if not info['exists']), status, contractor)

            # ìƒì„¸í™”ëœ í”„ë¡¬í”„íŠ¸ (AUDIT_FILTERS, ì£¼ê´€ì‚¬/ë¹„ì£¼ê´€ì‚¬, ì§„í–‰/ì¤€ê³µ ê¸°ì¤€ ë°˜ì˜)
            prompt = f"""
í”„ë¡œì íŠ¸ {project_id} ë¬¸ì„œ ë¶„ì„:
ë¶€ì„œ: {department}
í”„ë¡œì íŠ¸ëª…: {project_name}
ìƒíƒœ: {status}
ì£¼ê´€ì‚¬ ì—¬ë¶€: {contractor}

í˜„í™©:
- í™•ì¸ëœ ë¬¸ì„œ: {', '.join(existing_docs) if existing_docs else 'ì—†ìŒ'}
- ëˆ„ë½ëœ ë¬¸ì„œ: {', '.join(missing_docs) if missing_docs else 'ì—†ìŒ'}
- ê¸°ë³¸ ìœ„í—˜ë„: {risk_score}/100

ë‹¤ìŒ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ í‰ê°€í•´ì£¼ì„¸ìš”:
1. í˜„ì¬ ë¬¸ì„œí™” ìƒíƒœ (ìƒ/ì¤‘/í•˜) ë° ê·¸ ì´ìœ 
   - ì£¼ê´€ì‚¬: ëª¨ë“  ë¬¸ì„œ ìœ í˜•(ê³„ì•½ì„œ, ê³¼ì—…ì§€ì‹œì„œ, ì°©ìˆ˜ê³„, ê³µë™ë„ê¸‰í˜‘ì •, ì‹¤í–‰ì˜ˆì‚°, ì„±ê³¼í’ˆ(ë³´ê³ ì„œ), ì„±ê³¼í’ˆ(ë„ë©´), ì¤€ê³µê³„, ì‹¤ì ì¦ëª…, ìš©ì—­ìˆ˜í–‰í‰ê°€)ì´ 100% ì™„ë¹„ë˜ì–´ì•¼ í•¨ (ìœ„í—˜ë„ 0/100).
   - ë¹„ì£¼ê´€ì‚¬: ê³„ì•½ì„œ, ê³µë™ë„ê¸‰í˜‘ì •, ì„±ê³¼í’ˆ(ë³´ê³ ì„œ/ë„ë©´)ë§Œ í•„ìš” (ìœ„í—˜ë„ 50/100 ì´í•˜).
   - ì§„í–‰: ê³„ì•½ì„œ, ê³¼ì—…ì§€ì‹œì„œ, ì°©ìˆ˜ê³„, ê³µë™ë„ê¸‰í˜‘ì •, ì‹¤í–‰ì˜ˆì‚°ì´ ìš°ì„ ì ìœ¼ë¡œ ì¡´ì¬í•´ì•¼ í•˜ë©°, ë‚˜ë¨¸ì§€ ë¬¸ì„œëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ëˆ„ë½ ê°€ëŠ¥ (ìœ„í—˜ë„ 30~70/100).
   - ì¤€ê³µ: ëª¨ë“  ë¬¸ì„œ ìœ í˜•ì´ ì™„ë¹„ë˜ì–´ì•¼ í•¨ (ìœ„í—˜ë„ 0/100).

2. ê°€ì¥ ì‹œê¸‰í•œ ë³´ì™„ í•„ìš” ë¬¸ì„œì™€ ì´ìœ 
3. ìœ„í—˜ë„ ì ìˆ˜ ì¡°ì • í•„ìš”ì„± (ìˆë‹¤ë©´, êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ ì œì‹œ)

ê°„ë‹¨ëª…ë£Œí•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            cache_key = self._generate_cache_key(project_id, processed_documents)
            if cache_key in self._cache:
                logger.debug(f"ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜: {cache_key}")
                return self._cache[cache_key]

            # Gemini API í˜¸ì¶œ
            response_text = await self._call_gemini_with_retry(prompt)
            
            analysis = f"""ë¬¸ì„œ ë¶„ì„ ê²°ê³¼ ({datetime.now().strftime('%Y-%m-%d %H:%M')}):
{response_text}

ìš”ì•½:
- í™•ì¸ëœ ë¬¸ì„œ: {len([d for d in existing_docs if '(0ê°œ)' not in d])}ê°œ ìœ í˜•
- ëˆ„ë½ëœ ë¬¸ì„œ: {len([d for d in missing_docs if '(0ê°œ)' in d])}ê°œ ìœ í˜•
- ê¸°ë³¸ ìœ„í—˜ë„: {risk_score}/100
"""
            # ìºì‹œ ì €ì¥ (TTL 24ì‹œê°„ ì„¤ì •)
            self._cache[cache_key] = analysis
            asyncio.create_task(self._clear_cache_after_delay(cache_key, 24 * 60 * 60))  # 24ì‹œê°„ í›„ ìºì‹œ ì‚­ì œ

            # Discord ì•Œë¦¼ (ì„ íƒì , ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
            if DISCORD_WEBHOOK_URL:
                try:
                    notification = f"ğŸ” í”„ë¡œì íŠ¸ {project_id} ë¶„ì„ ì™„ë£Œ (ìœ„í—˜ë„: {risk_score}/100)"
                    async with session.post(DISCORD_WEBHOOK_URL, json={'content': notification}, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        await resp.read()
                except Exception as e:
                    logger.error(f"Discord ì•Œë¦¼ ì‹¤íŒ¨: {str(e)}")

            logger.info(f"í”„ë¡œì íŠ¸ {project_id} ë¶„ì„ ì™„ë£Œ, ìœ„í—˜ë„: {risk_score}/100")
            return analysis

        except Exception as e:
            error_msg = f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg)
            if DISCORD_WEBHOOK_URL:
                try:
                    async with session.post(DISCORD_WEBHOOK_URL, json={'content': f"âŒ {error_msg}"}, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                        await resp.read()
                except Exception as discord_e:
                    logger.error(f"Discord ì—ëŸ¬ ì•Œë¦¼ ì‹¤íŒ¨: {str(discord_e)}")
            return error_msg

        finally:
            if should_close and session:
                await session.close()

    async def _clear_cache_after_delay(self, cache_key: str, delay_seconds: int):
        """ì§€ì •ëœ ì‹œê°„ í›„ ìºì‹œ ì‚­ì œ"""
        await asyncio.sleep(delay_seconds)
        if cache_key in self._cache:
            del self._cache[cache_key]
            logger.debug(f"ìºì‹œ ì‚­ì œ: {cache_key}")

    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self.calculate_risk_score.cache_clear()
        logger.info("ë¶„ì„ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._session and not self._session.closed:
            await self._session.close()
            logger.info("HTTP ì„¸ì…˜ ì¢…ë£Œ")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
analyzer = DocumentAnalyzer()

async def analyze_with_gemini(project_data: dict, session: aiohttp.ClientSession = None) -> str:
    """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ë¥¼ ìœ„í•œ ë˜í¼"""
    return await analyzer.analyze_with_gemini(project_data, session)

def clear_analysis_cache():
    """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€ë¥¼ ìœ„í•œ ë˜í¼"""
    analyzer.clear_cache()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    async def test():
        # ì‹¤ì œ project_id="20180076" ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        test_data = {
            'project_id': '20180076',
            'department': 'ë„ë¡œ',
            'project_name': '20180076 ì˜ë½ê³µì› ì§„ì…ë„ë¡œ ê°œì„¤ê³µì‚¬ ì™¸ 4ê°œì†Œ ê¸°ë³¸ ë° ì‹¤ì‹œì„¤ê³„ìš©ì—­',
            'status': 'ì§„í–‰',
            'contractor': 'ì£¼ê´€ì‚¬',
            'documents': {
                'contract': {'exists': True, 'details': [{'name': '00 ê³„ì•½ê´€ë ¨.zip'}, {'name': '191223_ë³€ê²½ê³„ì•½ì„œ(1ì°¨).pdf'}, {'name': '200506_ë³€ê²½ê³„ì•½ì„œ(2ì°¨).pdf'}]},
                'specification': {'exists': True, 'details': [{'name': '00-ê³¼ì—…ë³„ ì—°ë½ì²˜ í˜„í™©-ver9.xls'}, {'name': 'ê³¼ì—… ì¶”ì§„í• ì‚¬í•­ ë° ë©”ëª¨ì‚¬í•­(2022.07.18).hwp'}, {'name': '20180533106-00_1527228162532_ê³¼ì—…ì§€ì‹œì„œ(ì˜ë½ê³µì› ì§„ì…ë„ë¡œë“± 5ê°œì†Œ)-ê¸°ë³¸ë°ì‹¤ì‹œì„¤ê³„.hwp'}]},
                'initiation': {'exists': True, 'details': [{'name': '180624 ì„¤ê³„ë‚´ì—­ì„œ(ë‚™ì°°)-ì°©ìˆ˜.xls'}, {'name': 'ì°©ìˆ˜ê³„(ì˜ë½ê³µì› ì§„ì…ë„ë¡œ)_18ë…„6ì›”26ì¼ ì°©ìˆ˜.pdf'}, {'name': 'ì°©ìˆ˜ê³„.hwp'}]},
                'agreement': {'exists': True, 'details': [{'name': 'ì—…ë¬´ë¶„ë‹´í•©ì˜ì„œ(ì˜ë½ê³µì›).pdf'}]},
                'budget': {'exists': True, 'details': [{'name': '2018076_ì˜ë½ê³µì› ì‹¤í–‰ì˜ˆì‚°.pdf'}, {'name': '231109 ì‹¤í–‰ì˜ˆì‚°ì„œ 2ì°¨(ì˜ë½ê³µì›).pdf'}, {'name': '231124 ì‹¤í–‰ì˜ˆì‚°ì„œ 4ì°¨(ì˜ë½ê³µì›).pdf'}]},
                'deliverable1': {'exists': True, 'details': [{'name': 'ìµœì¢… ì„±ê³¼í’ˆ ì œë³¸ í˜„í™©.hwp'}, {'name': '00 ë³´ê³ ì„œ_í‘œì§€(ì „ê¸°).hwp'}, {'name': '01 ì „ê¸°ë³´ê³ ì„œ.hwp'}]},
                'deliverable2': {'exists': True, 'details': [{'name': 'ì˜ë½ê³µì›ì§„ì…ë„ë¡œ ì „ê¸°ì„¤ê³„ë„ë©´.pdf'}, {'name': 'ì‹ ì°½2ì œ ì „ê¸°ì„¤ê³„ë„ë©´.pdf'}, {'name': 'ì „ê¸°ì„¤ê³„ë„ë©´(ë™êµ¬ì²­~ì¡°ëŒ€ì‚¬ê±°ë¦¬).pdf'}]},
                'completion': {'exists': True, 'details': [{'name': 'ì™¸ì£¼ë¹„ ì²­êµ¬ í•„ìˆ˜ì„œë¥˜(ê¸°ì„±,ì¤€ê³µ ê²€ì‚¬ì›)_ì˜ë½ê³µì›.hwp'}, {'name': 'ì™¸ì£¼ë¹„ ì²­êµ¬ í•„ìˆ˜ì„œë¥˜(ê¸°ì„±,ì¤€ê³µ ê²€ì‚¬ì›)_ì˜ë½ê³µì›-ì•„ì´ë¡œë“œì‘ì„±.pdf'}, {'name': 'ì™¸ì£¼ë¹„ ì²­êµ¬ í•„ìˆ˜ì„œë¥˜(ê¸°ì„±,ì¤€ê³µ ê²€ì‚¬ì›)_ì˜ë½ê³µì›.hwp'}]},
                'certificate': {'exists': True, 'details': [{'name': 'ì‹¤ì ì¦ëª…ì„œ_ì˜ë½ê³µì›(ìµœì¢…).hwp'}, {'name': 'ì‹¤ì ì¦ëª…ì„œ_ì˜ë½ê³µì›(ìµœì¢…)_ìˆ˜ì„±.hwp'}, {'name': 'ì‹¤ì ì¦ëª…ì„œ_ì˜ë½ê³µì›(ìµœì¢…)_ìˆ˜ì„±2.hwp'}]},
                'evaluation': {'exists': False, 'details': []}  # ìš©ì—­ìˆ˜í–‰í‰ê°€ ëˆ„ë½
            },
            'csv_data': {
                'Depart_ProjectID': '01010_20180076',
                'Depart': 'ë„ë¡œ',
                'Status': 'ì§„í–‰',
                'Contractor': 'ì£¼ê´€ì‚¬',
                'ProjectName': '20180076 ì˜ë½ê³µì› ì§„ì…ë„ë¡œ ê°œì„¤ê³µì‚¬ ì™¸ 4ê°œì†Œ ê¸°ë³¸ ë° ì‹¤ì‹œì„¤ê³„ìš©ì—­',
                'contract_exists': 1,
                'contract_count': 3,
                'specification_exists': 1,
                'specification_count': 3,
                'initiation_exists': 1,
                'initiation_count': 3,
                'agreement_exists': 1,
                'agreement_count': 1,
                'budget_exists': 1,
                'budget_count': 3,
                'deliverable1_exists': 1,
                'deliverable1_count': 3,
                'deliverable2_exists': 1,
                'deliverable2_count': 3,
                'completion_exists': 1,
                'completion_count': 3,
                'certificate_exists': 1,
                'certificate_count': 3,
                'evaluation_exists': 0,
                'evaluation_count': 0
            }
        }
        
        try:
            result = await analyze_with_gemini(test_data)
            logger.info(result)
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        finally:
            await analyzer.close()

    asyncio.run(test())
# python gemini.py