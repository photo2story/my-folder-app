import os
import asyncio
import aiofiles
from datetime import datetime
import json
import aiohttp
import re
from pathlib import Path
from search_project_data import ProjectDocumentSearcher
from gemini import analyze_with_gemini
from config import PROJECT_LIST_CSV, NETWORK_BASE_PATH, DISCORD_WEBHOOK_URL, STATIC_DATA_PATH
from config_assets import DOCUMENT_TYPES
import logging
import pandas as pd
import orjson
import time

# JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
RESULTS_DIR = os.path.join(os.path.dirname(STATIC_DATA_PATH), 'results')
os.makedirs(RESULTS_DIR, exist_ok=True)

logger = logging.getLogger(__name__)

class AuditService:
    def __init__(self):
        self.searcher = ProjectDocumentSearcher(verbose=False)

    async def _get_session(self):
        """aiohttp ì„¸ì…˜ ê´€ë¦¬"""
        return aiohttp.ClientSession()

    async def save_audit_result(self, result, department_code):
        """ê°ì‚¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        project_id = result['project_id']
        filename = f"audit_{department_code}_{project_id}.json"
        filepath = Path(RESULTS_DIR) / filename
        
        if not os.path.exists(RESULTS_DIR):
            os.makedirs(RESULTS_DIR, exist_ok=True)
        
        json_data = orjson.dumps(result, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS).decode('utf-8')
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(json_data)
        return str(filepath)

    async def send_to_discord(self, data):
        """ë””ìŠ¤ì½”ë“œë¡œ ê²°ê³¼ ì „ì†¡ (ê°„ì†Œí™”)"""
        if not DISCORD_WEBHOOK_URL:
            print(data if isinstance(data, str) else json.dumps(data, ensure_ascii=False, indent=2))
            return

        async with aiohttp.ClientSession() as session:
            try:
                message = (
                    f"ğŸ“‹ **Project Audit Result**\n"
                    f"ID: {data['project_id']}\n"
                    f"Department: {data['department']}\n"
                    f"Name: {data['project_name']}\n"
                    f"Path: {data['original_folder']}\n\n"
                    f"ğŸ“‘ Documents:\n"
                )

                found_docs = []
                missing_docs = []
                for doc_type, doc_list in data['documents'].items():
                    doc_name = DOCUMENT_TYPES[doc_type]['name']
                    if doc_list:  # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ë°œê²¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                        found_docs.append(f"{doc_name} ({len(doc_list)}ê°œ)")
                    else:
                        missing_docs.append(doc_name)

                if found_docs:
                    message += "âœ… Found:\n- " + "\n- ".join(found_docs) + "\n\n"
                if missing_docs:
                    message += "âŒ Missing:\n- " + "\n- ".join(missing_docs)

                message += f"\nâ° {data['timestamp']}"

                async with session.post(DISCORD_WEBHOOK_URL, json={'content': message}, timeout=10) as response:
                    if response.status != 204:
                        logger.warning(f"Webhook response status: {response.status}")
                        print(message)
            except Exception as e:
                logger.error(f"Discord send error: {str(e)}")
                print(message)

    async def get_project_info_by_dept(self, project_id, department_code=None):
        """í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ (ê°„ì†Œí™”)"""
        numeric_project_id = re.sub(r'[^0-9]', '', str(project_id))
        df = pd.read_csv(PROJECT_LIST_CSV, dtype={'department_code': str, 'project_id': str})
        
        if department_code:
            department_code = str(department_code).zfill(5)
            project = df[(df['project_id'] == numeric_project_id) & (df['department_code'].str.zfill(5) == department_code)]
        else:
            project = df[df['project_id'] == numeric_project_id]

        if len(project) == 0:
            logger.error(f"Project ID {numeric_project_id} not found for department {department_code}")
            return None

        row = project.iloc[0]
        full_path = os.path.join(NETWORK_BASE_PATH, str(row['original_folder']))
        return {
            'project_id': str(row['project_id']),
            'department_code': str(row['department_code']).zfill(5),
            'department_name': str(row['department_name']),
            'project_name': str(row['project_name']),
            'original_folder': full_path
        }

    async def audit_project(self, project_id, department_code=None, use_ai=False):
        start_time = time.time()
        try:
            logger.info(f"\n=== í”„ë¡œì íŠ¸ {project_id} (ID: {re.sub(r'[^0-9]', '', str(project_id))}) ê°ì‚¬ ì‹œì‘ ===")
            
            # í”„ë¡œì íŠ¸ ì •ë³´ ì¡°íšŒ
            project_info = await self.get_project_info_by_dept(project_id, department_code)
            if not project_info:
                raise ValueError(f'Project ID {project_id} not found for department {department_code}')
            
            # ë¬¸ì„œ ê²€ìƒ‰ (search_project_data.pyì—ì„œ ê°€ì ¸ì˜´, ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ìœ ì§€)
            search_start = time.time()
            search_result = await self.searcher.search_all_documents(project_info['project_id'], project_info['department_code'])
            search_time = time.time() - search_start
            
            if not search_result or 'documents' not in search_result:
                raise ValueError(f"ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_id}")
            
            documents = search_result['documents']  # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš© (search_project_data.pyì™€ ë™ì¼)
            total_files = sum(len(doc_list) for doc_list in documents.values() if doc_list)
            found_count = sum(1 for doc_list in documents.values() if doc_list)
            
            logger.info(f"\n=== ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({search_time:.2f}ì´ˆ) ===")
            logger.info(f"- ë°œê²¬ëœ ë¬¸ì„œ ìœ í˜•: {found_count}/{len(DOCUMENT_TYPES)}ê°œ")
            logger.info(f"- ì´ ë°œê²¬ íŒŒì¼ ìˆ˜: {total_files}ê°œ")
            
            # AI ë¶„ì„ (ì˜µì…˜)
            ai_analysis = None
            ai_time = 0
            if use_ai:
                logger.info("\n=== AI ë¶„ì„ ì‹œì‘ ===")
                ai_start = time.time()
                ai_input = {
                    'project_id': project_id,
                    'project_info': project_info,
                    'documents': documents
                }
                ai_analysis = await analyze_with_gemini(ai_input, await self._get_session())
                ai_time = time.time() - ai_start
                logger.info(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ===")
            
            # ê²°ê³¼ êµ¬ì„± (documentsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€)
            result = {
                'project_id': project_info['project_id'],
                'project_name': project_info['project_name'],
                'department': f"{project_info['department_code']}_{project_info['department_name']}",
                'documents': documents,  # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ìœ ì§€
                'original_folder': project_info['original_folder'],
                'ai_analysis': ai_analysis if use_ai else None,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'performance': {
                    'total_time': time.time() - start_time,
                    'search_time': search_time,
                    'ai_time': ai_time,
                    'save_time': 0
                }
            }
            
            # ê²°ê³¼ ì €ì¥
            save_start = time.time()
            json_path = await self.save_audit_result(result, project_info['department_code'])
            if json_path:
                result['json_file'] = json_path
                result['performance']['save_time'] = time.time() - save_start
                logger.info(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
            
            # Discordë¡œ ê²°ê³¼ ì „ì†¡
            await self.send_to_discord(result)
            
            logger.info(f"\n=== ê°ì‚¬ ì™„ë£Œ ({result['performance']['total_time']:.2f}ì´ˆ) ===")
            logger.info(f"- ì´ ì†Œìš” ì‹œê°„: {result['performance']['total_time']:.2f}ì´ˆ")
            logger.info(f"- ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„: {search_time:.2f}ì´ˆ")
            logger.info(f"- AI ë¶„ì„ ì‹œê°„: {ai_time:.2f}ì´ˆ")
            logger.info(f"- JSON ì €ì¥ ì‹œê°„: {result['performance']['save_time']:.2f}ì´ˆ")
            logger.info(f"- ë°œê²¬ëœ ë¬¸ì„œ ìœ í˜•: {found_count}/{len(DOCUMENT_TYPES)}ê°œ")
            logger.info(f"- ì´ ë°œê²¬ íŒŒì¼ ìˆ˜: {total_files}ê°œ")

            return result
            
        except Exception as e:
            error_msg = f"í”„ë¡œì íŠ¸ {project_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            error_result = {
                'error': str(e),
                'project_id': project_id,
                'department_code': department_code,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'performance': {'total_time': time.time() - start_time}
            }
            await self.send_to_discord(error_result)
            return error_result

    async def audit_multiple_projects(self, project_ids, department_codes, use_ai=False):
        """ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ë°°ì¹˜ ê°ì‚¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in project_ids]
        tasks = [asyncio.create_task(self.audit_project(pid, dept, use_ai)) for pid, dept in zip(numeric_project_ids, department_codes)]
        return await asyncio.gather(*tasks)

    async def process_audit_targets(self, filters=None, use_ai=False):
        """ê°ì‚¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°°ì¹˜ë¡œ ê°ì‚¬ ìˆ˜í–‰"""
        from audit_target_generator import select_audit_targets  # ë™ì  ì„í¬íŠ¸

        try:
            audit_targets_df, project_ids, department_codes = select_audit_targets(filters)
            numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in project_ids]
            results = await self.audit_multiple_projects(numeric_project_ids, department_codes, use_ai)
            
            audit_targets_df['AuditResult'] = [
                result.get('ai_analysis', 'No result') if 'error' not in result else f"Error: {result['error']}"
                for result in results
            ]
            
            output_csv = os.path.join(STATIC_DATA_PATH, 'audit_results.csv')
            audit_targets_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
            logger.info(f"ê°ì‚¬ ê²°ê³¼ê°€ {output_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ ìˆ˜: {len(audit_targets_df)}")
            return audit_targets_df, results
            
        except Exception as e:
            logger.error(f"ê°ì‚¬ ëŒ€ìƒ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None, None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰")
    parser.add_argument('--project-id', type=str, nargs='+', required=False, help="ê²€ìƒ‰í•  í”„ë¡œì íŠ¸ ID (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)")
    parser.add_argument('--department-code', type=str, nargs='+', default=None, help="ë¶€ì„œ ì½”ë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥, ì˜ˆ: 01010, 06010)")
    parser.add_argument('--use-ai', action='store_true', help="AI ë¶„ì„ ì‚¬ìš©")
    args = parser.parse_args()
    
    logger.info("=== í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ===")
    service = AuditService()
    
    if args.project_id:
        numeric_project_ids = [re.sub(r'[^0-9]', '', str(pid)) for pid in args.project_id]
        department_codes = args.department_code if args.department_code else [None] * len(numeric_project_ids)
        if len(department_codes) != len(numeric_project_ids):
            raise ValueError("ë¶€ì„œ ì½”ë“œì™€ í”„ë¡œì íŠ¸ IDì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        if len(numeric_project_ids) == 1:
            asyncio.run(service.audit_project(args.project_id[0], department_codes[0], args.use_ai))
        else:
            asyncio.run(service.audit_multiple_projects(numeric_project_ids, department_codes, args.use_ai))
    else:
        asyncio.run(service.process_audit_targets(use_ai=args.use_ai))
    
    logger.info("\n=== ê²€ìƒ‰ ì™„ë£Œ ===")
    
# python audit_service.py
# python audit_service.py --project-id 20180076 --department-code 01010 --use-ai
# python audit_service.py --project-id 20240178 --department-code 06010 --use-ai
# python audit_service.py --project-id 20240178 --use-ai