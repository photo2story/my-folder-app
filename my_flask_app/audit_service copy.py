# /my_flask_app/audit_service.py

# /my_flask_app/audit_service.py

import os
import asyncio
import aiofiles
from datetime import datetime
import json
import aiohttp
import re
from pathlib import Path
from search_project_data import ProjectDocumentSearcher
from gemini import analyze_with_gemini
from config import PROJECT_LIST_CSV, NETWORK_BASE_PATH, DISCORD_WEBHOOK_URL, STATIC_DATA_PATH, STATIC_PATH, CONTRACT_STATUS_CSV, RESULTS_DIR
from config_assets import DOCUMENT_TYPES, DEPARTMENT_MAPPING, DEPARTMENT_NAMES, AUDIT_FILTERS
import logging
import pandas as pd
import orjson
import time
from get_project import get_project_info
import ast
from audit_message import send_audit_to_discord, send_audit_status_to_discord
from git_operations import sync_files_to_github  # git_operations ì„í¬íŠ¸

# JSON íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
os.makedirs(RESULTS_DIR, exist_ok=True)

logger = logging.getLogger(__name__)

class AuditService:
    def __init__(self):
        self.searcher = ProjectDocumentSearcher(verbose=False)
        self._session = None

    async def _get_session(self):
        """aiohttp ì„¸ì…˜ ê´€ë¦¬"""
        if self._session is None:
            self._session = aiohttp.ClientSession()
        return self._session

    async def close(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        if self._session is not None:
            await self._session.close()
            self._session = None

    async def save_audit_result(self, result, department_code):
        """ê°ì‚¬ ê²°ê³¼ë¥¼ ë¶€ì„œë³„ í´ë”ì— JSONìœ¼ë¡œ ì €ì¥í•˜ë©°, ì˜ëª»ëœ ë¬¸ìì—´ì„ ë³€í™˜"""
        project_id = result['project_id']
        department = result.get('department', f"{department_code}_Unknown").replace('.', '_')  # .ì„ _ë¡œ êµì²´
        if not re.match(r'^\d+_\w+$', department):
            logger.warning(f"Invalid department format: {department}, normalizing...")
            department = re.sub(r'[^0-9a-zA-Z_]', '_', department)

        department_folder = os.path.join(RESULTS_DIR, department)
        filename = f"audit_{project_id}.json"
        filepath = os.path.join(department_folder, filename)

        if not os.path.exists(department_folder):
            os.makedirs(department_folder, exist_ok=True)
            logger.info(f"Created department folder: {department_folder}")

        def fix_document_details(details):
            if isinstance(details, list):
                corrected_details = []
                for item in details:
                    if isinstance(item, str):  # ë¬¸ìì—´ì¸ ê²½ìš° JSON ë³€í™˜ ì‹œë„
                        try:
                            corrected_item = json.loads(item.replace("'", "\""))
                        except json.JSONDecodeError:
                            corrected_item = item  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                        corrected_details.append(corrected_item)
                    else:
                        corrected_details.append(item)  # ë”•ì…”ë„ˆë¦¬ ë“±ì€ ê·¸ëŒ€ë¡œ ì¶”ê°€
                return corrected_details
            return details

        for doc_type, doc_info in result.get('documents', {}).items():
            if 'details' in doc_info:
                doc_info['details'] = fix_document_details(doc_info['details'])
                logger.debug(f"Fixed {doc_type} details: {doc_info['details']}")

        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(result, ensure_ascii=False, indent=2))

        logger.info(f"âœ… ê°ì‚¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")

        # ì €ì¥ í›„ GitHubì— ì—…ë¡œë“œ
        await sync_files_to_github(filepath)  # íŠ¹ì • íŒŒì¼ë§Œ ì—…ë¡œë“œ
        logger.info(f"âœ… ê°ì‚¬ ê²°ê³¼ GitHubì— ì—…ë¡œë“œ ì™„ë£Œ: {filepath}")

        return filepath

    async def _send_single_to_discord(self, data, ctx=None):
        """ë‹¨ì¼ ê°ì‚¬ ê²°ê³¼ë¥¼ Discordë¡œ ì „ì†¡ (ë‚´ë¶€ í•¨ìˆ˜)"""
        if not isinstance(data, dict):
            logger.error(f"Invalid audit data format: {data}")
            return False

        if data.get('project_id') == 'Unknown' and data.get('department') == 'Unknown':
            logger.warning("Skipping Unknown project result")
            return True

        message = (
            f"ğŸ“‹ **Project Audit Result**\n"
            f"ID: {data.get('project_id', 'Unknown')}\n"
            f"Department: {data.get('department', data.get('department_code', 'Unknown'))}\n"
            f"Name: {data.get('project_name') or 'Project ' + str(data.get('project_id', 'Unknown'))}\n"
            f"Status: {data.get('status', 'Unknown')}\n"
            f"Contractor: {data.get('contractor', 'Unknown')}\n"
            f"Path: {data.get('project_path', 'Unknown')}\n\n"
            f"ğŸ“‘ Documents:\n"
        )

        found_docs = []
        missing_docs = []
        documents = data.get('documents', {})
        
        for doc_type in DOCUMENT_TYPES.keys():
            doc_info = documents.get(doc_type, {'exists': False, 'details': []})
            doc_name = DOCUMENT_TYPES.get(doc_type, {}).get('name', doc_type)
            if doc_info.get('exists', False):
                count = len(doc_info.get('details', []))
                found_docs.append(f"{doc_name} ({count}ê°œ)")
            else:
                missing_docs.append(f"{doc_type} (0ê°œ)")

        if found_docs:
            message += "âœ… Found:\n- " + "\n- ".join(found_docs) + "\n\n"
        if missing_docs:
            message += "âŒ Missing:\n- " + "\n- ".join(missing_docs) + "\n\n"

        if 'ai_analysis' in data and data['ai_analysis']:
            message += f"\nğŸ¤– AI Analysis:\n{data['ai_analysis']}"

        message += f"\nâ° {data.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}"

        try:
            if ctx:
                await ctx.send(message)
                logger.info(f"Sent audit result to Discord channel: {message}")
            elif DISCORD_WEBHOOK_URL:
                async with aiohttp.ClientSession() as session:
                    async with session.post(DISCORD_WEBHOOK_URL, json={'content': message}, timeout=10) as response:
                        if response.status != 204:
                            logger.warning(f"Webhook response status: {response.status}")
                            return False
                    logger.info("Audit result successfully sent to Discord webhook")
                    return True
            else:
                print(message)
                return True
        except Exception as e:
            logger.error(f"Error sending to Discord: {str(e)}")
            if ctx:
                await ctx.send(f"âŒ ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
            return False

    def load_contract_data(self):
        """contract_status.csvì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ë¡œë“œ"""
        try:
            df = pd.read_csv(CONTRACT_STATUS_CSV, encoding='utf-8-sig')
            if 'ì‚¬ì—…ì½”ë“œ' not in df.columns or 'PMë¶€ì„œ' not in df.columns or 'ì§„í–‰ìƒíƒœ' not in df.columns or 'ì‚¬ì—…ëª…' not in df.columns or 'ì£¼ê´€ì‚¬' not in df.columns:
                raise ValueError("CSV must contain 'ì‚¬ì—…ì½”ë“œ', 'PMë¶€ì„œ', 'ì§„í–‰ìƒíƒœ', 'ì‚¬ì—…ëª…', and 'ì£¼ê´€ì‚¬' columns")

            def map_department(pm_dept):
                dept_name = pm_dept.strip()
                dept_code = DEPARTMENT_MAPPING.get(dept_name, '99999')
                return dept_code

            df['ProjectID'] = df['ì‚¬ì—…ì½”ë“œ'].apply(lambda x: str(x))  # ìˆ«ì ì œê±° ëŒ€ì‹  ì›ë˜ ê°’ ìœ ì§€
            df['Depart_Code'] = df['PMë¶€ì„œ'].apply(map_department)
            df['Depart'] = df['Depart_Code'].map(DEPARTMENT_NAMES).fillna(df['PMë¶€ì„œ'])
            df['Contractor'] = df['ì£¼ê´€ì‚¬'].apply(lambda x: 'ì£¼ê´€ì‚¬' if x == 'ì£¼ê´€ì‚¬' else 'ë¹„ì£¼ê´€ì‚¬')
            return df[['ProjectID', 'Depart_Code', 'Depart', 'ì§„í–‰ìƒíƒœ', 'ì‚¬ì—…ëª…', 'Contractor']]
        except Exception as e:
            logger.error(f"Failed to load contract data from {CONTRACT_STATUS_CSV}: {str(e)}")
            return pd.DataFrame()

    async def search_projects_by_id(self, project_id, department_code=None):
        """project_idë§Œ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œì íŠ¸ ì •ë³´ ë° í´ë”ë¥¼ ì°¾ì•„ ê²€ìƒ‰"""
        numeric_project_id = re.sub(r'[^0-9]', '', str(project_id))  # ê²€ìƒ‰ìš© ìˆ«ì ID
        projects = []

        # audit_targets_new.csvì—ì„œ ì›ë˜ ProjectID ê°€ì ¸ì˜¤ê¸°
        csv_path = os.path.join(STATIC_DATA_PATH, 'audit_targets_new.csv')
        original_project_id = project_id  # ê¸°ë³¸ê°’ì€ ì…ë ¥ëœ project_id
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            project_row = df[df['ProjectID'].str.replace(r'[^0-9]', '', regex=True) == numeric_project_id]
            if not project_row.empty:
                original_project_id = project_row['ProjectID'].iloc[0]  # ì›ë˜ í˜•ì‹ (ì˜ˆ: C20240178)
                logger.debug(f"Found original ProjectID: {original_project_id}")
        except Exception as e:
            logger.error(f"Error reading audit_targets_new.csv: {str(e)}")

        # contract_status.csvì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        contract_df = self.load_contract_data()
        contract_match = contract_df[contract_df['ProjectID'] == original_project_id]
        
        if not contract_match.empty:
            row = contract_match.iloc[0]
            dept_code = row['Depart_Code']
            dept_name = row['Depart']
            project_name = row['ì‚¬ì—…ëª…']
            status = row['ì§„í–‰ìƒíƒœ']
            contractor = row['Contractor']
        else:
            loop = asyncio.get_event_loop()
            project_info = await loop.run_in_executor(None, lambda: get_project_info(project_id))
            if not project_info:
                logger.warning(f"Project ID {numeric_project_id} not found, using defaults")
                dept_code = '99999'
                dept_name = 'Unknown'
                project_name = f'Project {numeric_project_id}'
                status = 'Unknown'
                contractor = 'Unknown'
            else:
                dept_code = project_info['department_code']
                dept_name = project_info['department_name']
                project_name = project_info['project_name']
                status = project_info.get('status', 'Unknown')
                contractor = project_info.get('contractor', 'Unknown')

        if not department_code:
            department_code = dept_code

        # audit_targets_new.csvì—ì„œ search_folder í™•ì¸
        search_folder = None
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            project_row = df[df['ProjectID'] == original_project_id]
            if not project_row.empty:
                search_folder = str(project_row['search_folder'].iloc[0])
                if search_folder in ["No folder", "No directory"]:
                    logger.warning(f"Project {original_project_id} has No folder, searching default path")
                    search_folder = None
        except Exception as e:
            logger.error(f"Error reading audit_targets_new.csv: {str(e)}")

        # project_list.csvì—ì„œ original_folder í™•ì¸
        if os.path.exists(PROJECT_LIST_CSV):
            try:
                df_projects = pd.read_csv(PROJECT_LIST_CSV, encoding='utf-8-sig')
                df_projects['project_id'] = df_projects['project_id'].apply(lambda x: re.sub(r'[^0-9]', '', str(x)))
                project_row_pl = df_projects[df_projects['project_id'] == numeric_project_id]
                if not project_row_pl.empty:
                    original_folder = project_row_pl['original_folder'].iloc[0]
                    folder_path = os.path.join(NETWORK_BASE_PATH, original_folder)
                    if os.path.exists(folder_path):
                        search_folder = folder_path
                        logger.info(f"Found project folder: {search_folder}")
            except Exception as e:
                logger.error(f"Error reading project_list.csv: {str(e)}")

        # ê¸°ë³¸ ê²½ë¡œ ê²€ìƒ‰
        if not search_folder or search_folder in ["No folder", "No directory"]:
            base_paths = [
                os.path.join(NETWORK_BASE_PATH, numeric_project_id),
                os.path.join(NETWORK_BASE_PATH, f"Y{numeric_project_id}"),
                os.path.join(NETWORK_BASE_PATH, f"{numeric_project_id}_")
            ]
            for path in base_paths:
                if os.path.exists(path):
                    search_folder = path
                    logger.info(f"Found default folder: {search_folder}")
                    break
            if not search_folder:
                logger.warning(f"No folder found for Project {original_project_id}")
                return [{
                    'project_id': original_project_id,  # ì›ë˜ project_id ì‚¬ìš©
                    'department_code': dept_code,
                    'department_name': dept_name,
                    'project_name': project_name,
                    'original_folder': None,
                    'status': status,
                    'contractor': contractor,
                    'documents': {doc_type: {'exists': False, 'details': []} for doc_type in DOCUMENT_TYPES}
                }]

        if search_folder not in ["No folder", "No directory"]:
            if not await asyncio.to_thread(os.path.exists, search_folder):
                logger.error(f"Folder does not exist: {search_folder}")
                return [{
                    'project_id': original_project_id,
                    'department_code': dept_code,
                    'department_name': dept_name,
                    'project_name': project_name,
                    'original_folder': search_folder,
                    'status': status,
                    'contractor': contractor,
                    'documents': {doc_type: {'exists': False, 'details': []} for doc_type in DOCUMENT_TYPES}
                }]

            logger.debug(f"Searching project_id: {project_id} in folder: {search_folder}")
            search_result = await self.searcher.process_single_project(project_id)
            logger.debug(f"Raw search result: {search_result}")

            if search_result:
                documents = search_result.get('documents', {})
                logger.debug(f"Documents from search_result: {documents}")
                processed_documents = {}
                for doc_type, type_info in DOCUMENT_TYPES.items():
                    doc_data = documents.get(doc_type, [])
                    if isinstance(doc_data, list) and doc_data:
                        details = doc_data
                        processed_documents[doc_type] = {
                            'exists': len(details) > 0,
                            'details': details
                        }
                        logger.debug(f"Processed {doc_type}: exists={len(details) > 0}, details={details}")
                    else:
                        processed_documents[doc_type] = {
                            'exists': False,
                            'details': []
                        }
                        logger.debug(f"Processed {doc_type}: exists=False, details=[]")

                total_files = sum(len(doc_info['details']) for doc_info in processed_documents.values())
                logger.debug(f"Total files calculated: {total_files}")
                projects.append({
                    'project_id': original_project_id,
                    'department_code': dept_code,
                    'department_name': dept_name,
                    'project_name': project_name,
                    'original_folder': search_folder,
                    'status': status,
                    'contractor': contractor,
                    'documents': processed_documents
                })
                logger.info(f"Found project path: {search_folder}, Total files: {total_files}")
            else:
                logger.warning(f"No search result returned for {original_project_id}")
                projects.append({
                    'project_id': original_project_id,
                    'department_code': dept_code,
                    'department_name': dept_name,
                    'project_name': project_name,
                    'original_folder': search_folder,
                    'status': status,
                    'contractor': contractor,
                    'documents': {doc_type: {'exists': False, 'details': []} for doc_type in DOCUMENT_TYPES}
                })

        logger.debug(f"Returning projects: {projects}")
        return projects

    async def audit_project(self, project_id, department_code=None, use_ai=False, ctx=None):
        """ë‹¨ì¼ í”„ë¡œì íŠ¸ ê°ì‚¬"""
        start_time = time.time()
        try:
            logger.info(f"\n=== í”„ë¡œì íŠ¸ {project_id} (ID: {re.sub(r'[^0-9]', '', str(project_id))}) ê°ì‚¬ ì‹œì‘ ===")
            if ctx:
                await send_audit_status_to_discord(ctx, f"ğŸ” í”„ë¡œì íŠ¸ {project_id} ê°ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

            # audit_targets_new.csvì—ì„œ ì›ë˜ ProjectID ê°€ì ¸ì˜¤ê¸°
            csv_path = os.path.join(STATIC_DATA_PATH, 'audit_targets_new.csv')
            original_project_id = project_id
            try:
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                numeric_project_id = re.sub(r'[^0-9]', '', str(project_id))
                project_row = df[df['ProjectID'].str.replace(r'[^0-9]', '', regex=True) == numeric_project_id]
                if not project_row.empty:
                    original_project_id = project_row['ProjectID'].iloc[0]  # "C20240178"
                    logger.debug(f"Found original ProjectID: {original_project_id}")
            except Exception as e:
                logger.error(f"Error reading audit_targets_new.csv: {str(e)}")

            projects = await self.search_projects_by_id(project_id)
            if not projects:
                contract_df = self.load_contract_data()
                contract_match = contract_df[contract_df['ProjectID'] == original_project_id]
                if not contract_match.empty:
                    row = contract_match.iloc[0]
                    dept_code = row['Depart_Code']
                    dept_name = row['Depart']
                    project_name = row['ì‚¬ì—…ëª…']
                    status = row['ì§„í–‰ìƒíƒœ']
                    contractor = row['Contractor']
                    folder_path = None

                    result = {
                        'project_id': original_project_id,
                        'project_name': project_name,
                        'department': f"{dept_code}_{dept_name}",
                        'status': status,
                        'contractor': contractor,
                        'documents': {doc_type: {'exists': False, 'details': []} for doc_type in DOCUMENT_TYPES},
                        'project_path': folder_path,
                        'ai_analysis': None,
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'performance': {
                            'total_time': 0,
                            'search_time': 0,
                            'ai_time': 0,
                            'save_time': 0
                        }
                    }
                    
                    search_time = time.time() - start_time
                    ai_analysis = None
                    ai_time = 0
                    if use_ai:
                        if ctx:
                            await ctx.send(f"\n=== AI ë¶„ì„ ì‹œì‘ ({dept_name}) ===")
                        logger.info(f"\n=== AI ë¶„ì„ ì‹œì‘ ({dept_name}) ===")
                        ai_start = time.time()
                        ai_input = {
                            'project_id': original_project_id,
                            'department': dept_name,
                            'project_name': project_name,
                            'status': status,
                            'contractor': contractor,
                            'documents': result['documents'],
                            'csv_data': {
                                'Depart_ProjectID': f"{dept_code}_{re.sub(r'[^0-9]', '', str(project_id))}",
                                'Depart': dept_name,
                                'Status': status,
                                'Contractor': contractor,
                                'ProjectName': project_name,
                                **{f'{doc_type}_exists': 0 for doc_type in DOCUMENT_TYPES.keys()},
                                **{f'{doc_type}_count': 0 for doc_type in DOCUMENT_TYPES.keys()}
                            }
                        }
                        try:
                            ai_analysis = await analyze_with_gemini(ai_input, await self._get_session())
                        except Exception as e:
                            logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                            ai_analysis = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                        ai_time = time.time() - ai_start
                        if ctx:
                            await ctx.send(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ({dept_name})\nAI Analysis: {ai_analysis}")
                        logger.info(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ({dept_name})\nAI Analysis: {ai_analysis}")
                    
                    result['ai_analysis'] = ai_analysis if use_ai else None
                    result['performance']['search_time'] = search_time
                    result['performance']['ai_time'] = ai_time
                    
                    save_start = time.time()
                    json_path = await self.save_audit_result(result, dept_code)
                    if json_path:
                        result['result_file'] = json_path
                        result['performance']['save_time'] = time.time() - save_start
                        if ctx:
                            await ctx.send(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ ({dept_name}): {json_path}")
                        logger.info(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ ({dept_name}): {json_path}")
                    
                    result['performance']['total_time'] = time.time() - start_time
                    try:
                        await send_audit_to_discord(result)
                    except Exception as e:
                        logger.error(f"âŒ ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
                    
                    return [result]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
                else:
                    raise ValueError(f"Project ID {project_id} not found")

            all_results = []
            for project_info in projects:
                result = {
                    'project_id': project_info.get('project_id'),
                    'project_name': project_info.get('project_name'),
                    'department': f"{project_info.get('department_code')}_{project_info.get('department_name')}",
                    'status': project_info.get('status', 'Unknown'),
                    'contractor': project_info.get('contractor', 'Unknown'),
                    'documents': project_info['documents'].copy(),
                    'project_path': project_info.get('original_folder'),
                    'ai_analysis': None,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'performance': {
                        'total_time': 0,
                        'search_time': 0,
                        'ai_time': 0,
                        'save_time': 0
                    }
                }
                
                search_time = time.time() - start_time
                logger.debug(f"Documents for project {project_info['project_id']}: {result['documents']}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                
                csv_data = {
                    'Depart_ProjectID': f"{project_info['department_code']}_{re.sub(r'[^0-9]', '', str(project_info['project_id']))}",
                    'Depart': project_info['department_name'],
                    'Status': project_info['status'],
                    'Contractor': project_info['contractor'],
                    'ProjectName': project_info['project_name'],
                    'contract_exists': 1 if result['documents'].get('contract', {}).get('exists', False) else 0,
                    'contract_count': len(result['documents'].get('contract', {}).get('details', [])),
                    'specification_exists': 1 if result['documents'].get('specification', {}).get('exists', False) else 0,
                    'specification_count': len(result['documents'].get('specification', {}).get('details', [])),
                    'initiation_exists': 1 if result['documents'].get('initiation', {}).get('exists', False) else 0,
                    'initiation_count': len(result['documents'].get('initiation', {}).get('details', [])),
                    'agreement_exists': 1 if result['documents'].get('agreement', {}).get('exists', False) else 0,
                    'agreement_count': len(result['documents'].get('agreement', {}).get('details', [])),
                    'budget_exists': 1 if result['documents'].get('budget', {}).get('exists', False) else 0,
                    'budget_count': len(result['documents'].get('budget', {}).get('details', [])),
                    'deliverable1_exists': 1 if result['documents'].get('deliverable1', {}).get('exists', False) else 0,
                    'deliverable1_count': len(result['documents'].get('deliverable1', {}).get('details', [])),
                    'deliverable2_exists': 1 if result['documents'].get('deliverable2', {}).get('exists', False) else 0,
                    'deliverable2_count': len(result['documents'].get('deliverable2', {}).get('details', [])),
                    'completion_exists': 1 if result['documents'].get('completion', {}).get('exists', False) else 0,
                    'completion_count': len(result['documents'].get('completion', {}).get('details', [])),
                    'certificate_exists': 1 if result['documents'].get('certificate', {}).get('exists', False) else 0,
                    'certificate_count': len(result['documents'].get('certificate', {}).get('details', [])),
                    'evaluation_exists': 1 if result['documents'].get('evaluation', {}).get('exists', False) else 0,
                    'evaluation_count': len(result['documents'].get('evaluation', {}).get('details', []))
                }
                logger.debug(f"Gemini AI CSV ë°ì´í„°: {csv_data}")

                ai_analysis = None
                ai_time = 0
                if use_ai:
                    if ctx:
                        await ctx.send(f"\n=== AI ë¶„ì„ ì‹œì‘ ({project_info['department_name']}) ===")
                    logger.info(f"\n=== AI ë¶„ì„ ì‹œì‘ ({project_info['department_name']}) ===")
                    ai_start = time.time()
                    ai_input = {
                        'project_id': project_info['project_id'],
                        'department': project_info['department_name'],
                        'project_name': project_info['project_name'],
                        'status': project_info['status'],
                        'contractor': project_info['contractor'],
                        'documents': result['documents'],
                        'csv_data': csv_data
                    }
                    try:
                        ai_analysis = await analyze_with_gemini(ai_input, await self._get_session())
                    except Exception as e:
                        logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
                        ai_analysis = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    ai_time = time.time() - ai_start
                    if ctx:
                        await ctx.send(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ({project_info['department_name']})\nAI Analysis: {ai_analysis}")
                    logger.info(f"=== AI ë¶„ì„ ì™„ë£Œ ({ai_time:.2f}ì´ˆ) ({project_info['department_name']})\nAI Analysis: {ai_analysis}")
                
                result['ai_analysis'] = ai_analysis if use_ai else None
                result['performance']['search_time'] = search_time
                result['performance']['ai_time'] = ai_time
                
                save_start = time.time()
                json_path = await self.save_audit_result(result, project_info['department_code'])
                if json_path:
                    result['result_file'] = json_path
                    result['performance']['save_time'] = time.time() - save_start
                    if ctx:
                        await ctx.send(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ ({project_info['department_name']}): {json_path}")
                    logger.info(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ ({project_info['department_name']}): {json_path}")
                
                result['performance']['total_time'] = time.time() - start_time
                try:
                    await send_audit_to_discord(result)
                except Exception as e:
                    logger.error(f"âŒ ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
                
                all_results.append(result)
            
            if ctx:
                await ctx.send(f"\n=== ëª¨ë“  ë¶€ì„œì— ëŒ€í•œ ê°ì‚¬ ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ) ===")
            
            logger.info(f"\n=== ëª¨ë“  ë¶€ì„œì— ëŒ€í•œ ê°ì‚¬ ì™„ë£Œ ({time.time() - start_time:.2f}ì´ˆ) ===")
            logger.info(f"- ì´ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            logger.info(f"- ë°œê²¬ëœ ë¶€ì„œ: {len(projects)}ê°œ")
            total_files = sum(len(doc_info.get('details', [])) for p in projects for doc_info in p['documents'].values() if isinstance(doc_info, dict))
            logger.info(f"- ì´ ë°œê²¬ íŒŒì¼ ìˆ˜: {total_files}")
            logger.info(f"- ë°œê²¬ëœ ë¬¸ì„œ ìœ í˜• ìˆ˜: {len({doc_type for p in projects for doc_type in p['documents'].keys() if p['documents'][doc_type].get('exists', False)})}")

            valid_results = [r for r in all_results if r.get('project_id') != 'Unknown']
            return valid_results if valid_results else all_results  # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        except Exception as e:
            error_msg = f"í”„ë¡œì íŠ¸ {project_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            if ctx:
                await ctx.send(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
            error_result = {
                'error': str(e),
                'project_id': original_project_id,
                'department_code': department_code,
                'department': 'Unknown' if not department_code else f"{department_code}_{DEPARTMENT_NAMES.get(department_code, 'Unknown')}",
                'status': 'Unknown',
                'contractor': 'Unknown',
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'performance': {
                    'total_time': time.time() - start_time,
                    'search_time': 0,
                    'ai_time': 0,
                    'save_time': 0
                }
            }
            try:
                await send_audit_to_discord(error_result)
            except Exception as e:
                logger.error(f"âŒ ë””ìŠ¤ì½”ë“œ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
            return [error_result]  # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    async def audit_multiple_projects(self, project_ids, use_ai=False):
        """ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ë°°ì¹˜ ê°ì‚¬"""
        tasks = [asyncio.create_task(self.audit_project(pid, None, use_ai)) for pid in project_ids]  # ì›ë˜ ID ì „ë‹¬
        return await asyncio.gather(*tasks)

    async def process_audit_targets(self, filters=None, use_ai=False, skip_no_folder=False):
        """ê°ì‚¬ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ë°°ì¹˜ ê°ì‚¬"""
        from audit_target_generator import select_audit_targets

        try:
            data_dir = os.path.join(STATIC_DATA_PATH)
            os.makedirs(data_dir, exist_ok=True)
            if not os.access(data_dir, os.W_OK):
                logger.warning(f"No write permission for {data_dir}, attempting to fix...")
                import stat
                os.chmod(data_dir, stat.S_IWRITE | stat.S_IREAD)

            audit_targets_df, project_ids, department_codes = select_audit_targets(filters or AUDIT_FILTERS)
            if audit_targets_df.empty or 'ProjectID' not in audit_targets_df.columns:
                if 'Depart_ProjectID' in audit_targets_df.columns:
                    audit_targets_df['ProjectID'] = audit_targets_df['Depart_ProjectID'].apply(lambda x: x.split('_')[-1])
                    logger.warning(f"Generated ProjectID from Depart_ProjectID")
                else:
                    logger.error("No valid ProjectID or Depart_ProjectID column")
                    return None, None

            logger.info(f"ğŸ“Š ì´ {len(project_ids)}ê°œ í”„ë¡œì íŠ¸ ì²˜ë¦¬ ì‹œì‘...")
            results = []

            for idx, project_id in enumerate(project_ids):
                progress = f"({idx + 1}/{len(project_ids)})"
                if idx % 10 == 0:
                    logger.info(f"ğŸ”„ ì§„í–‰ì¤‘... {progress}")
                
                logger.info(f"ğŸ” í”„ë¡œì íŠ¸ {project_id} ê°ì‚¬ ì‹œì‘...")
                try:
                    csv_path = os.path.join(STATIC_DATA_PATH, 'audit_targets_new.csv')
                    df = pd.read_csv(csv_path, encoding='utf-8-sig')
                    project_row = df[df['ProjectID'] == str(project_id)]
                    search_folder = project_row['search_folder'].iloc[0] if not project_row.empty else None
                    
                    if skip_no_folder and search_folder in ["No folder", "No directory"]:
                        logger.info(f"Skipping project {project_id} (No folder, skip_no_folder=True)")
                        continue

                    if search_folder in ["No folder", "No directory"]:
                        result = {
                            "project_id": project_id,
                            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "documents_found": 0,
                            "risk_level": 0,
                            "missing_docs": 0,
                            "department": project_row['Depart'].iloc[0] if not project_row.empty else "Unknown",
                            "status": project_row['Status'].iloc[0] if not project_row.empty else "Unknown",
                            "contractor": project_row['Contractor'].iloc[0] if not project_row.empty else "Unknown",
                            "project_name": project_row['ProjectName'].iloc[0] if not project_row.empty else "Unknown",
                            "result": "0,0,0,0,0,0,0 (Folder missing)"
                        }
                        results.append(result)
                        logger.info(f"âœ… í”„ë¡œì íŠ¸ {project_id} ì™„ë£Œ: 0,0,0,0,0,0,0 (Folder missing) {progress}")
                    else:
                        result = await self.audit_project(project_id, None, use_ai, None)
                        if 'error' not in result[0]:
                            results.append(result[0])
                            logger.info(f"âœ… í”„ë¡œì íŠ¸ {project_id} ì™„ë£Œ: {result[0].get('timestamp')} {progress}")
                        else:
                            logger.error(f"âŒ í”„ë¡œì íŠ¸ {project_id} ì‹¤íŒ¨: {result[0]['error']} {progress}")
                    
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"Error processing project {project_id}: {str(e)}")
                    continue
            
            audit_targets_df['AuditResult'] = [
                result.get('result', 'No result') if 'error' not in result else f"Error: {result['error']}"
                for result in results
            ]
            output_csv = os.path.join(STATIC_DATA_PATH, 'audit_results.csv')
            audit_targets_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
            logger.info(f"ê°ì‚¬ ê²°ê³¼ ì €ì¥: {output_csv}, ì´ í”„ë¡œì íŠ¸ ìˆ˜: {len(audit_targets_df)}")
            return audit_targets_df, results
            
        except Exception as e:
            logger.error(f"ê°ì‚¬ ëŒ€ìƒ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None, None

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰")
    parser.add_argument('--project-id', type=str, nargs='+', required=False, help="ê²€ìƒ‰í•  í”„ë¡œì íŠ¸ ID")
    parser.add_argument('--department-code', type=str, nargs='+', default=None, help="ë¶€ì„œ ì½”ë“œ")
    parser.add_argument('--use-ai', action='store_true', help="AI ë¶„ì„ ì‚¬ìš©")
    parser.add_argument('--skip-no-folder', action='store_true', help="No folder í”„ë¡œì íŠ¸ íŒ¨ìŠ¤")
    args = parser.parse_args()
    
    async def main():
        logger.info("=== í”„ë¡œì íŠ¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ===")
        service = AuditService()
        try:
            if args.project_id:
                if len(args.project_id) == 1:
                    await service.audit_project(args.project_id[0], None, args.use_ai, ctx=None)
                else:
                    await service.audit_multiple_projects(args.project_id, args.use_ai)
            else:
                await service.process_audit_targets(use_ai=args.use_ai, skip_no_folder=args.skip_no_folder)
        finally:
            await service.close()
            logger.info("\n=== ê²€ìƒ‰ ì™„ë£Œ ===")
    
    asyncio.run(main())
    
# python audit_service.py
# python audit_service.py --project-id 20180076 --use-ai
# python audit_service.py --project-id 20240178 --use-ai
# python audit_service.py --project-id 20190088 --use-ai # ì¤€ê³µí´ë”,9999
# python audit_service.py --project-id 20190088 --use-ai # ì¤€ê³µí´ë”,9999
# python audit_service.py --project-id 20240001 --use-ai 